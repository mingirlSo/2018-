{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "#import lightgbm as lgb\n",
    "import time\n",
    "import pickle\n",
    "import scipy.stats as st\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV,RandomizedSearchCV   #Perforing grid search\n",
    "from sklearn.preprocessing import LabelBinarizer,LabelEncoder,StandardScaler,MinMaxScaler,robust_scale\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import make_scorer, mean_squared_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "??sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE = mean_squared_error(y_true, y_pred)\n",
    "#RMSE= mean_squared_error(y,y_pred)**0.5 \n",
    "#RMSE =np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual_values, predicted_values, convertExp=True):\n",
    "    \"\"\"\n",
    "    - root mean squared log error는 error를 로그화값으로 변환하고, 제곱하고, 평균을 내고, 루트를 씌웁니다.\n",
    "    - skewness를 해결하기 위해 np.log1p를 했기 때문에, 값을 예측할 때 이를 다시 변환해서 처리해주는 것이 필요합니다. \n",
    "    \"\"\"\n",
    "    if convertExp==True:\n",
    "        predicted_values = np.exp(predicted_values),\n",
    "        actual_values = np.exp(actual_values)\n",
    "    # 위에서 계산한 예측값에서 실제값을 빼주고 제곱을 해준다.\n",
    "    difference = np.square(predicted_values - actual_values)\n",
    "    return np.sqrt(difference.mean())\n",
    "RMSE = make_scorer(rmse, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error_(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions) ** 0.5\n",
    "RMSE = make_scorer(root_mean_squared_error_, greater_is_better=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5801, 14)\n",
      "(5800, 13)\n"
     ]
    }
   ],
   "source": [
    "train= pd.read_csv(\"train_in.csv\",encoding=\"cp949\") #nohash+scale0 / #nohash+scale1\n",
    "test = pd.read_csv(\"test_in.csv\",encoding=\"cp949\")\n",
    "print(train.shape)\n",
    "print(test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5801, 12)\n",
      "(5800, 12)\n"
     ]
    }
   ],
   "source": [
    "#train/test 데이터 세팅 + DMatrix화  #나중에 apartment_id도 뺼지 고려.\n",
    "train_x, train_y = train.drop([\"date\",\"number\"],axis=1) ,train[\"number\"]\n",
    "print(train_x.shape)\n",
    "#test\n",
    "#data_dmatrix = xgb.DMatrix(data=train_x,label=train_y)\n",
    "test_s=test.drop(\"date\",axis=1)\n",
    "print(test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 & 정규화 \n",
    "# 선형 변환을 적용하여 전체 자료의 분포를 평균 0, 분산 1이 되도록 만드는 과정이다\n",
    "# 최적화 과정에서의 안정성 및 수렴 속도를 향상\n",
    "#scaler = MinMaxScaler() #정규화 (때때로 제한)\n",
    "#scaler = StandardScaler() #표준화 (이상치에 좋음)\n",
    "#이상치에 좋은 robust_scale\n",
    "#train_x = scaler.fit_transform(train_x)\n",
    "#train_x2 = scaler.fit_transform(train_x2)\n",
    "#test_s =scaler.transform(test_s)\n",
    "#test_b = scaler.transform(test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y값 로그변환 (이미 R에서 함 ) / 서울: log / 부산 log2\n",
    "#train_y =np.log2(train_y)\n",
    "#train_y2 =np.log2(train_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 & Search모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?xgb.XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch  params\n",
    "#https://github.com/albertkklam/XGBRegressor/blob/master/XGBRegressor.ipynb\n",
    "#1.max_depth/ min_child_weigh\n",
    "#1-2. 위를 기준으로 +-1, +-1~0.5 미세조정\n",
    "#2.gamma \n",
    "#3.subsample/ colsample_bytree : 0.6~ 1\n",
    "#3-2. 위를 기준으로 미세조정 0.01단위 \n",
    "#4.reg_alpha/ reg_lambda\n",
    "#4-2. *0.2/*0.5/*2/*5\n",
    "params = {\n",
    "        \"max_depth\": [6, 9, 12], #int\n",
    "        \"learning_rate\": [0.1, 0.2, 0.3], #float 좀 키워도 됄듯 0.005, 0.05 \n",
    "        #'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        #'colsample_bylevel': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"min_child_weight\": [1, 3], #int\n",
    "        \"gamma\": [0, 0.5, 1.0, 2.0], #float\n",
    "        \"reg_alpha\": [1.0, 5.0],  #float\n",
    "        \"reg_lambda\": [1.0, 5.0, 10.0],\n",
    "        \"n_estimators\": [150, 200,300] #int \n",
    "}\n",
    "3*3*2*4*2*3*3\n",
    "#params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n",
    "#'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomsearch params \n",
    "#rvs\n",
    "one_to_left = st.beta(10, 1) #베타 연속 확률 변수 \n",
    "from_zero_positive = st.expon(0, 50) #지수 연속 난수 변수\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(100, 200), #균일 이산 확률 변수.\n",
    "    \"max_depth\": st.randint(3, 10),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4), #균일 연속 확률 변수\n",
    "    #\"colsample_bytree\": one_to_left,\n",
    "    #\"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "#서울데이터: 1학습: 5~6분/ 부산데이터 1학습: 1~2분\n",
    "#param_grid = dict(penalty=['l1', 'l2'], dual=[False], tol=[0.0001], C=[1.0], fit_intercept=[True], \n",
    "#                  intercept_scaling=[1], class_weight=[None], random_state=[None], solver=['liblinear'], \n",
    "#                  max_iter=[100], multi_class=['ovr'], verbose=[0], warm_start=[False], n_jobs=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = xgb.XGBRegressor(#subsample=0.7, \n",
    "                             colsample_bytree=0.8,\n",
    "                             objective= 'reg:linear', \n",
    "                             tree_method='hist' , scale_pos_weight=1, seed=1234) #nthread 안쓰임 \n",
    "                                                              #tree method :auto/exact/approx/hist/gpu_exact/gpu_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV 설정 \n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=4,shuffle=True,random_state=1234) #이게 default일듯 \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkfold = RepeatedKFold(n_splits=4, n_repeats=2,random_state=1234)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skfold = StratifiedKFold(n_splits=3)#,shuffle = False) #계층별 교차검증   타깃값 클래스 비율을 고려\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "rskfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "#?StratifiedKFold #찾아보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GridSearchCV(estimator,\n",
    "#                     param_grid = params, n_jobs=1 , scoring='neg_mean_squared_error',iid=False, cv=kfold,verbose=5) \n",
    "#'neg_mean_squared_error'/RMSE /n_jobs /verbose 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomizedSearchCV(estimator,param_distributions = params, \n",
    "                           n_jobs=1, n_iter=40,scoring=\"neg_mean_squared_error\",iid=False, cv=4, verbose=True) \n",
    "#fit_params={'early_stopping_rounds':20,'eval_set':[(X,y)]},cv=kfold\n",
    "#'neg_mean_squared_error'/RMSE /n_jobs:1외엔 잘안댕.  /n_iter: 학습하는 랜덤의 조합수  /verbose: n_iter*cv/verbose가 10정도 되게.\n",
    "#20 cv:3 ->3시간 / 40분\n",
    "#30 30/cv:3 :5시간 50 /1시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomizedSearchCV\n",
    "#RandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True, \n",
    "#refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GridSearchCV\n",
    "#GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', \n",
    "#error_score='raise')\n",
    "#https://www.programcreek.com/python/example/104786/sklearn.grid_search.GridSearchCV\n",
    "#https://wikidocs.net/17858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서울 학습모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 40 candidates, totalling 160 fits\n",
      "[08:12:02] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:02] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:02] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:02] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:02] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:03] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:04] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:04] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:05] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:05] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:06] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:06] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:06] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:07] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:07] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:08] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:08] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:09] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:10] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:10] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:11] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:11] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:11] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:12] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:12] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:13] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:13] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:14] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:14] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:15] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:15] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:16] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:16] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:16] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:17] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:17] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:18] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:18] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:19] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:19] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:20] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:20] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:20] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:21] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:21] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:21] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:24] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:24] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:25] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:26] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:26] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:27] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:27] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:28] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:28] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:28] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:29] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:29] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:29] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:30] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:30] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:30] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:31] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:31] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:32] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:32] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:33] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:34] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:34] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:35] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:36] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:36] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:37] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:37] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:38] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:38] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:39] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:39] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:12:40] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:40] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:41] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:41] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:41] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:42] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:42] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:43] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:44] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:44] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:45] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:45] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:45] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:46] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:46] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:47] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:47] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:48] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:49] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:50] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:50] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:51] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:53] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:54] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:55] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:56] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:57] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:57] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:57] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:58] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:58] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:59] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:59] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:12:59] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:00] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:01] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:02] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:03] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:03] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:04] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:04] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:05] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:05] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:05] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:06] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:06] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:07] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:08] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:10] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:11] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:11] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:12] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:13] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:14] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:14] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:15] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:16] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:17] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:17] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:18] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:18] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:19] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:19] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:20] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:20] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:21] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:21] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:22] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:23] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:24] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:25] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:26] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[08:13:27] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:13:28] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "Wall time: 1min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1234,\n",
       "       silent=True, subsample=1, tree_method='hist'),\n",
       "          fit_params={}, iid=False, n_iter=40, n_jobs=1,\n",
       "          param_distributions={'max_depth': [6, 9, 12], 'learning_rate': [0.1, 0.2, 0.3], 'min_child_weight': [1, 3], 'gamma': [0, 0.5, 1.0, 2.0], 'reg_alpha': [1.0, 5.0], 'reg_lambda': [1.0, 5.0, 10.0], 'n_estimators': [150, 200, 300]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='neg_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#저장\n",
    "with open('xgb_model_s_1213_1.sav', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "model = pickle.load(open('xgb_model_s_1213_1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_: {'reg_lambda': 1.0, 'reg_alpha': 1.0, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 12, 'learning_rate': 0.2, 'gamma': 2.0}\n"
     ]
    }
   ],
   "source": [
    "#print('best_score_:', model.best_score_) #\n",
    "print('best_params_:', model.best_params_) #\n",
    "#print('best_estimator_:', model.best_estimator_)\n",
    "\n",
    "#'reg_lambda': 5.0, 'reg_alpha': 0.1, 'n_estimators': 200, 'min_child_weight': 7, 'max_depth': 15, \n",
    "#'learning_rate': 0.1, 'gamma': 5.0, 'colsample_bytree': 0.6, 'colsample_bylevel': 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -876.13641, std: 149.38469, params: {'reg_lambda': 0.1, 'reg_alpha': 5.0, 'n_estimators': 50, 'min_child_weight': 3, 'max_depth': 9, 'learning_rate': 0.4, 'gamma': 0},\n",
       " mean: -7580.46646, std: 501.66704, params: {'reg_lambda': 10.0, 'reg_alpha': 50.0, 'n_estimators': 150, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 5.0},\n",
       " mean: -864.94331, std: 140.54546, params: {'reg_lambda': 0.1, 'reg_alpha': 0.1, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 15, 'learning_rate': 0.2, 'gamma': 1.0},\n",
       " mean: -882.49723, std: 130.34433, params: {'reg_lambda': 0.1, 'reg_alpha': 5.0, 'n_estimators': 200, 'min_child_weight': 0.5, 'max_depth': 9, 'learning_rate': 0.4, 'gamma': 9.0},\n",
       " mean: -748.17671, std: 157.23437, params: {'reg_lambda': 50.0, 'reg_alpha': 1.0, 'n_estimators': 150, 'min_child_weight': 0.5, 'max_depth': 12, 'learning_rate': 0.3, 'gamma': 2.0},\n",
       " mean: -755.58795, std: 166.67320, params: {'reg_lambda': 50.0, 'reg_alpha': 1.0, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 12, 'learning_rate': 0.4, 'gamma': 1.0},\n",
       " mean: -905.98151, std: 133.62268, params: {'reg_lambda': 1.0, 'reg_alpha': 1.0, 'n_estimators': 100, 'min_child_weight': 0.5, 'max_depth': 15, 'learning_rate': 0.4, 'gamma': 2.0},\n",
       " mean: -6939.42205, std: 390.26398, params: {'reg_lambda': 10.0, 'reg_alpha': 10.0, 'n_estimators': 200, 'min_child_weight': 0.5, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 0.5},\n",
       " mean: -806.55818, std: 129.58051, params: {'reg_lambda': 0.1, 'reg_alpha': 10.0, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 9, 'learning_rate': 0.3, 'gamma': 2.0},\n",
       " mean: -850.81702, std: 135.56029, params: {'reg_lambda': 0.1, 'reg_alpha': 10.0, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 18, 'learning_rate': 0.3, 'gamma': 1.0}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=np.log2(1+10)\n",
    "#np.exp2(a)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date       number\n",
      "0  20180911  1914.193604\n",
      "1  20180914  1914.193604\n",
      "2  20180912  1914.193604\n",
      "3  20180916  1914.193604\n",
      "4  20180920  1914.193604\n",
      "(5800, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = model.best_estimator_.predict(test_s)\n",
    "pred =pd.DataFrame(pred)\n",
    "final = pd.concat([test['date'],pred],axis=1)\n",
    "final.columns = ['date', 'number']\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test['date'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['date', 'number']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final['transaction_real_price'] = np.exp2(final['transaction_real_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:/Users/Admin/Desktop/so1.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부산 학습모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 53.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1234,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params={}, iid=False, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'max_depth': [6, 9, 12, 15, 18], 'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5], 'min_child_weight': [0.5, 1, 3, 5, 7, 10], 'gamma': [0, 0.5, 1.0, 2.0, 5.0, 9.0], 'reg_alpha': [0.1, 1.0, 5.0, 10.0, 50.0], 'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0], 'n_estimators': [50, 100, 150, 200]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='neg_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(train_x2,train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#저장\n",
    "with open('xgb_model_b_12131_1.sav', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "model = pickle.load(open('xgb_model_b_1213_1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_: {'reg_lambda': 10.0, 'reg_alpha': 1.0, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 15, 'learning_rate': 0.1, 'gamma': 9.0}\n"
     ]
    }
   ],
   "source": [
    "#print('best_score_:', model.best_score_) #\n",
    "print('best_params_:', model.best_params_) #\n",
    "#print('best_estimator_:', model.best_estimator_)\n",
    "\n",
    "#'reg_lambda': 0.1, 'reg_alpha': 5.0, 'n_estimators': 150, 'min_child_weight': 0.5, 'max_depth': 9, \n",
    "#'learning_rate': 0.1, 'gamma': 1.0, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -99478810.67773, std: 18055914.46490, params: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 400},\n",
       " mean: -99846999.16894, std: 18106563.59580, params: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 600},\n",
       " mean: -98798320.89296, std: 15546140.79421, params: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 400},\n",
       " mean: -99021720.46451, std: 15594170.71449, params: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 600},\n",
       " mean: -98730439.36205, std: 17693207.76468, params: {'gamma': 0, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 400},\n",
       " mean: -99108860.99240, std: 18076457.18505, params: {'gamma': 0, 'learning_rate': 0.05, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 600},\n",
       " mean: -96565591.09976, std: 17509019.40817, params: {'gamma': 0, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 400},\n",
       " mean: -96760379.04294, std: 17587849.39475, params: {'gamma': 0, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 600}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       key  transaction_real_price\n",
      "0  1253422             150003216.0\n",
      "1  1369751             147191680.0\n",
      "2  1389544             120483928.0\n",
      "3  1394472             390055200.0\n",
      "4  1395869             230603584.0\n",
      "(1281, 2)\n"
     ]
    }
   ],
   "source": [
    "#pred = grid.predict(test)\n",
    "pred = model.best_estimator_.predict(test_b)\n",
    "pred =pd.DataFrame(pred)\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test2['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final['transaction_real_price'] = np.exp2(final['transaction_real_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:/Users/Admin/Desktop/ZPER 4차/submission/xgb1216_b1.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale만 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2637, 102)\n",
      "(1281, 94)\n"
     ]
    }
   ],
   "source": [
    "#scale만 한 데이터\n",
    "#서울 데이터 \n",
    "train= pd.read_csv(\"train1718_s+nohash+scale0.csv\") #nohash+scale0 / #nohash+scale1\n",
    "test = pd.read_csv(\"test1718_s+nohash+scale0.csv\") \n",
    "#부산 데이터\n",
    "train2= pd.read_csv(\"train1718_b+nohash+scale0.csv\")\n",
    "test2 = pd.read_csv(\"test1718_b+nohash+scale0.csv\")\n",
    "#train/test 데이터 세팅 + DMatrix화  #나중에 apartment_id도 뺼지 고려.\n",
    "train_x, train_y = train.drop([\"key\",\"Y\"],axis=1) ,train[\"Y\"]\n",
    "train_x2, train_y2 = train2.drop([\"key\",\"Y\"],axis=1),train2[\"Y\"]\n",
    "#train.drop(['id', 'target'], inplace=True, axis=1)\n",
    "#test\n",
    "#data_dmatrix = xgb.DMatrix(data=train_x,label=train_y)\n",
    "test_s =test.drop(\"key\",axis=1)\n",
    "test_b =test2.drop(\"key\",axis=1)\n",
    "print(test_s.shape)\n",
    "print(test_b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 181.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 4min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1234,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params={}, iid=False, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'max_depth': [6, 9, 12, 15, 18], 'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5], 'min_child_weight': [0.5, 1, 3, 5, 7, 10], 'gamma': [0, 0.5, 1.0, 2.0, 5.0, 9.0], 'reg_alpha': [0.1, 1.0, 5.0, 10.0, 50.0], 'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0], 'n_estimators': [50, 100, 150, 200]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='neg_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_model_s_1218_scale0.sav', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      key  transaction_real_price\n",
      "0  462533            1.217938e+09\n",
      "1  764018            5.816460e+08\n",
      "2  813528            2.897185e+08\n",
      "3  845097            4.901704e+08\n",
      "4  856338            1.852312e+08\n",
      "(2637, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = model.best_estimator_.predict(test_s)\n",
    "pred =pd.DataFrame(pred)\n",
    "final = pd.concat([test['key'],pred],axis=1)\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:/Users/Admin/Desktop/ZPER 4차/submission/xgb1216_s2.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x2' is not defined"
     ]
    }
   ],
   "source": [
    "%time model.fit(train_x2,train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['apartment_id', '(Intercept)_1', 'sigungu11140_1', 'sigungu11170_1', 'sigungu11200_1', 'sigungu11215_1', 'sigungu11230_1', 'sigungu11260_1', 'sigungu11290_1', 'sigungu11305_1', 'sigungu11320_1', 'sigungu11350_1', 'sigungu11380_1', 'sigungu11410_1', 'sigungu11440_1', 'sigungu11470_1', 'sigungu11500_1', 'sigungu11530_1', 'sigungu11545_1', 'sigungu11560_1', 'sigungu11590_1', 'sigungu11620_1', 'sigungu11650_1', 'sigungu11680_1', 'sigungu11710_1', 'sigungu11740_1', 'sigungu26110_1', 'sigungu26140_1', 'sigungu26170_1', 'sigungu26200_1', 'sigungu26230_1', 'sigungu26260_1', 'sigungu26290_1', 'sigungu26320_1', 'sigungu26350_1', 'sigungu26380_1', 'sigungu26410_1', 'sigungu26440_1', 'sigungu26470_1', 'sigungu26500_1', 'sigungu26530_1', 'sigungu26710_1', 'sigungu28245_1', '(Intercept)_2', 'transaction_date11~20_2', 'transaction_date21~31_2', '(Intercept)_3', 'heat_typedistrict_3', 'heat_typeindividual_3', 'heat_typeN_3', '(Intercept)_4', 'heat_fuelgas_4', 'heat_fuelN_4', '(Intercept)_5', 'front_door_structuremixed_5', 'front_door_structureN_5', 'front_door_structurestairway_5', '(Intercept)_6', 'year2018_6', 'year2009_6', 'year2013_6', 'year2014_6', 'year2015_6', 'year2016_6', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 'sKJ', 'sUS', 'sKC', 'sDL', 'sND', '(Intercept)', 'disfac1', 'disfac2', 'disfac3', 'exclusive_use_area', 'floor', 'latitude', 'longitude', 'total_parking_capacity_in_site', 'total_household_count_in_sites', 'apartment_building_count_in_sites', 'tallest_building_in_sites', 'lowest_building_in_sites', 'total_household_count_of_area_type', 'room_count', 'bathroom_count', 'area2', 'yeardif', 'min_dis1', 'min_dis2', 'mean_lat', 'mean_lon', 'median_lat', 'median_lon'] ['apartment_id', '(Intercept)_1', 'sigungu11140_1', 'sigungu11170_1', 'sigungu11200_1', 'sigungu11215_1', 'sigungu11230_1', 'sigungu11260_1', 'sigungu11290_1', 'sigungu11305_1', 'sigungu11320_1', 'sigungu11350_1', 'sigungu11380_1', 'sigungu11410_1', 'sigungu11440_1', 'sigungu11470_1', 'sigungu11500_1', 'sigungu11530_1', 'sigungu11545_1', 'sigungu11560_1', 'sigungu11590_1', 'sigungu11620_1', 'sigungu11650_1', 'sigungu11680_1', 'sigungu11710_1', 'sigungu11740_1', 'sigungu26110_1', 'sigungu26140_1', 'sigungu26170_1', 'sigungu26200_1', 'sigungu26230_1', 'sigungu26260_1', 'sigungu26290_1', 'sigungu26320_1', 'sigungu26350_1', 'sigungu26380_1', 'sigungu26410_1', 'sigungu26440_1', 'sigungu26470_1', 'sigungu26500_1', 'sigungu26530_1', 'sigungu26710_1', 'sigungu28245_1', '(Intercept)_2', 'transaction_date11~20_2', 'transaction_date21~31_2', '(Intercept)_3', 'heat_typedistrict_3', 'heat_typeindividual_3', 'heat_typeN_3', '(Intercept)_4', 'heat_fuelgas_4', 'heat_fuelN_4', '(Intercept)_5', 'front_door_structuremixed_5', 'front_door_structureN_5', 'front_door_structurestairway_5', '(Intercept)_6', 'year2018_6', 'year2009_6', 'year2013_6', 'year2014_6', 'year2015_6', 'year2016_6', 'b1', 'b2', 'b3', 'b4', 'bd', 'bk', '(Intercept)', 'disfac1', 'disfac2', 'disfac3', 'exclusive_use_area', 'floor', 'latitude', 'longitude', 'total_parking_capacity_in_site', 'total_household_count_in_sites', 'apartment_building_count_in_sites', 'tallest_building_in_sites', 'lowest_building_in_sites', 'total_household_count_of_area_type', 'room_count', 'bathroom_count', 'area2', 'yeardif', 'min_dis1', 'min_dis2', 'mean_lat', 'mean_lon', 'median_lat', 'median_lon']\nexpected sUS, sKJ, s8, s4, s7, s3, s2, sDL, sND, s5, s1, s6, s9, sKC in input data\ntraining data did not have the following fields: bk, bd, b3, b4, b2, b1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-affdc43d2386>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#best_est = grid.best_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#final = final.reset_index()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit)\u001b[0m\n\u001b[0;32m    345\u001b[0m         return self.get_booster().predict(test_dmatrix,\n\u001b[0;32m    346\u001b[0m                                           \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                                           ntree_limit=ntree_limit)\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1477\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['apartment_id', '(Intercept)_1', 'sigungu11140_1', 'sigungu11170_1', 'sigungu11200_1', 'sigungu11215_1', 'sigungu11230_1', 'sigungu11260_1', 'sigungu11290_1', 'sigungu11305_1', 'sigungu11320_1', 'sigungu11350_1', 'sigungu11380_1', 'sigungu11410_1', 'sigungu11440_1', 'sigungu11470_1', 'sigungu11500_1', 'sigungu11530_1', 'sigungu11545_1', 'sigungu11560_1', 'sigungu11590_1', 'sigungu11620_1', 'sigungu11650_1', 'sigungu11680_1', 'sigungu11710_1', 'sigungu11740_1', 'sigungu26110_1', 'sigungu26140_1', 'sigungu26170_1', 'sigungu26200_1', 'sigungu26230_1', 'sigungu26260_1', 'sigungu26290_1', 'sigungu26320_1', 'sigungu26350_1', 'sigungu26380_1', 'sigungu26410_1', 'sigungu26440_1', 'sigungu26470_1', 'sigungu26500_1', 'sigungu26530_1', 'sigungu26710_1', 'sigungu28245_1', '(Intercept)_2', 'transaction_date11~20_2', 'transaction_date21~31_2', '(Intercept)_3', 'heat_typedistrict_3', 'heat_typeindividual_3', 'heat_typeN_3', '(Intercept)_4', 'heat_fuelgas_4', 'heat_fuelN_4', '(Intercept)_5', 'front_door_structuremixed_5', 'front_door_structureN_5', 'front_door_structurestairway_5', '(Intercept)_6', 'year2018_6', 'year2009_6', 'year2013_6', 'year2014_6', 'year2015_6', 'year2016_6', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 'sKJ', 'sUS', 'sKC', 'sDL', 'sND', '(Intercept)', 'disfac1', 'disfac2', 'disfac3', 'exclusive_use_area', 'floor', 'latitude', 'longitude', 'total_parking_capacity_in_site', 'total_household_count_in_sites', 'apartment_building_count_in_sites', 'tallest_building_in_sites', 'lowest_building_in_sites', 'total_household_count_of_area_type', 'room_count', 'bathroom_count', 'area2', 'yeardif', 'min_dis1', 'min_dis2', 'mean_lat', 'mean_lon', 'median_lat', 'median_lon'] ['apartment_id', '(Intercept)_1', 'sigungu11140_1', 'sigungu11170_1', 'sigungu11200_1', 'sigungu11215_1', 'sigungu11230_1', 'sigungu11260_1', 'sigungu11290_1', 'sigungu11305_1', 'sigungu11320_1', 'sigungu11350_1', 'sigungu11380_1', 'sigungu11410_1', 'sigungu11440_1', 'sigungu11470_1', 'sigungu11500_1', 'sigungu11530_1', 'sigungu11545_1', 'sigungu11560_1', 'sigungu11590_1', 'sigungu11620_1', 'sigungu11650_1', 'sigungu11680_1', 'sigungu11710_1', 'sigungu11740_1', 'sigungu26110_1', 'sigungu26140_1', 'sigungu26170_1', 'sigungu26200_1', 'sigungu26230_1', 'sigungu26260_1', 'sigungu26290_1', 'sigungu26320_1', 'sigungu26350_1', 'sigungu26380_1', 'sigungu26410_1', 'sigungu26440_1', 'sigungu26470_1', 'sigungu26500_1', 'sigungu26530_1', 'sigungu26710_1', 'sigungu28245_1', '(Intercept)_2', 'transaction_date11~20_2', 'transaction_date21~31_2', '(Intercept)_3', 'heat_typedistrict_3', 'heat_typeindividual_3', 'heat_typeN_3', '(Intercept)_4', 'heat_fuelgas_4', 'heat_fuelN_4', '(Intercept)_5', 'front_door_structuremixed_5', 'front_door_structureN_5', 'front_door_structurestairway_5', '(Intercept)_6', 'year2018_6', 'year2009_6', 'year2013_6', 'year2014_6', 'year2015_6', 'year2016_6', 'b1', 'b2', 'b3', 'b4', 'bd', 'bk', '(Intercept)', 'disfac1', 'disfac2', 'disfac3', 'exclusive_use_area', 'floor', 'latitude', 'longitude', 'total_parking_capacity_in_site', 'total_household_count_in_sites', 'apartment_building_count_in_sites', 'tallest_building_in_sites', 'lowest_building_in_sites', 'total_household_count_of_area_type', 'room_count', 'bathroom_count', 'area2', 'yeardif', 'min_dis1', 'min_dis2', 'mean_lat', 'mean_lon', 'median_lat', 'median_lon']\nexpected sUS, sKJ, s8, s4, s7, s3, s2, sDL, sND, s5, s1, s6, s9, sKC in input data\ntraining data did not have the following fields: bk, bd, b3, b4, b2, b1"
     ]
    }
   ],
   "source": [
    "pred = model.best_estimator_.predict(test_b)\n",
    "pred =pd.DataFrame(pred)\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test2['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:/Users/Admin/Desktop/ZPER 4차/submission/xgb1216_b2.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale+log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale+log/sqrt변환한 데이터\n",
    "#서울 데이터 \n",
    "train= pd.read_csv(\"train1718_s+nohash+scale0.csv\") #nohash+scale0 / #nohash+scale1\n",
    "test = pd.read_csv(\"test1718_s+nohash+scale0.csv\") \n",
    "#부산 데이터\n",
    "train2= pd.read_csv(\"train1718_b+nohash+scale0.csv\")\n",
    "test2 = pd.read_csv(\"test1718_b+nohash+scale0.csv\")\n",
    "#train/test 데이터 세팅 + DMatrix화  #나중에 apartment_id도 뺼지 고려.\n",
    "train_x, train_y = train.drop([\"key\",\"Y\"],axis=1) ,train[\"Y\"]\n",
    "#train.drop(['id', 'target'], inplace=True, axis=1)\n",
    "#test\n",
    "#data_dmatrix = xgb.DMatrix(data=train_x,label=train_y)\n",
    "test_s =test.drop(\"key\",axis=1)\n",
    "test_b =test2.drop(\"key\",axis=1)\n",
    "print(test_s.shape)\n",
    "print(test_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.best_estimator_.predict(test_s)\n",
    "pred =pd.DataFrame(pred)\n",
    "final = pd.concat([test['key'],pred],axis=1)\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['transaction_real_price'] = np.exp(final['transaction_real_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:/Users/Admin/Desktop/ZPER 4차/submission/xgb1216_s3.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit(train_x2,train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.best_estimator_.predict(test_b)\n",
    "pred =pd.DataFrame(pred)\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test2['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['transaction_real_price'] = np.exp2(final['transaction_real_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"C:/Users/Admin/Desktop/ZPER 4차/submission/xgb1216_b3.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 및 파라미터 설정함수 후 한번에 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_jobs 활성화 (훨씬 느리다 그냥 쓰지말자. )\n",
    "def somefunction():\n",
    "    estimator = xgb.XGBRegressor(subsample=0.7, colsample_bytree=0.7,\n",
    "                             objective= 'reg:linear', scale_pos_weight=1, seed=1234) \n",
    "    params = {\n",
    "        \"max_depth\": [6, 8, 10, 12 ,15], #int\n",
    "        \"learning_rate\": [0.005, 0.01, 0.05, 0.1, 0.2, 0.3], #float\n",
    "        #'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        #'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        \"min_child_weight\": [0.5, 1, 3, 5, 7, 10], #int\n",
    "        \"gamma\": [0, 0.25, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 9.0], #float\n",
    "        \"reg_alpha\": [0.1, 1.0, 5.0, 10.0, 50.0],  #float\n",
    "        \"reg_lambda\": [0.1, 1.0, 5.0, 10.0, 50.0],\n",
    "        \"n_estimators\": [100, 125, 150, 175, 200] #int \n",
    "    }\n",
    "    model = RandomizedSearchCV(estimator,param_distributions = params, \n",
    "                           n_jobs=1, n_iter=30,scoring=\"neg_mean_squared_error\",iid=False, cv=3, verbose=9) #n_jobs=-1\n",
    "    %time model.fit(train_x,train_y)\n",
    "    return model\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "#    somefunction()\n",
    "somefunction() #-1 :16~20분.(모델하나당 5~6.5분)\n",
    "               # 1 :11분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb.plot_tree(estimator,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(estimator)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet (릿지 + 라쏘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#릿지는 scale에 민감 (규제가 있는 모형은 대부분.)\n",
    "#alpha: 패널티 항을 곱하는 상수. 기본값은 1.0\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "#model = Ridge(alpha=1,solver=\"cholesky\") #\"sag\"\n",
    "#라쏘\n",
    "#model1 =Lasso(alpha=0.1)\n",
    "#엘라스틱 \n",
    "#model2 = ElasticNet(alpha=0.1,l1_ratio=0.5) #l1_ratio =0 :릿지 / =1:라쏘\n",
    "estimator =ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {'alpha': [1,0.5,0.1,0.05,0.01,0.05,0.001],\n",
    "         #'alpha': np.arange(1e-4,1e-3,1e-4), #a lower alpha is more likely to overfit (C=1/alpha) np.arange(1e-3,1e-2,1e-3)\n",
    "         'l1_ratio': np.arange(0.1,1.0,0.1),  #L1(라쏘): 가중치의 절대값이 최소가 되게, 특정 변수들은 모델 생성시 사용 X\n",
    "         'max_iter':[50,100,200,500,1000]}    #L2(릿지): 라쏘의 경우 가중치가 0이 되지만, 릿지는 가중치가 0에 가깝게 됨.\n",
    "                 #특성이 많은데 그중 일부분만 중요하다면 라쏘가, 특성의 중요도가 전체적으로 비슷하다면 릿지가 좀 더 괜찮은 모델을 찾아줌 \n",
    "    \n",
    "#20 *cv3: 2분\n",
    "#500   -> 50분?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(estimator,\n",
    "                     param_grid = params, n_jobs=1 , scoring='neg_mean_squared_error',iid=False, cv=3,verbose=True) #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomizedSearchCV(estimator,\n",
    "#            param_distributions = params, n_jobs=1, n_iter=20,scoring=\"neg_mean_squared_error\",iid=False, cv=3,verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elastic_model_s_1211.sav', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "model = pickle.load(open('elastic_model_s_1211.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_: {'alpha': 0.0001, 'l1_ratio': 0.5, 'max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "print('best_params_:', model.best_params_) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      key  transaction_real_price\n",
      "0  462533            1.415346e+09\n",
      "1  764018            9.590182e+08\n",
      "2  813528            4.312191e+08\n",
      "3  845097            3.060653e+08\n",
      "4  856338           -6.910453e+07\n",
      "(2637, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = model.best_estimator_.predict(test_s)\n",
    "pred =pd.DataFrame(pred)\n",
    "final = pd.concat([test['key'],pred],axis=1)\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"elastic1211_s.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.fit(train_x2,train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elastic_model_b_1211.sav', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불러오기\n",
    "model = pickle.load(open('elastic_model_b_1211.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_: {'max_iter': 200, 'l1_ratio': 0.9, 'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print('best_params_:', model.best_params_) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       key  transaction_real_price\n",
      "0  1253422            2.274509e+08\n",
      "1  1369751            1.705920e+08\n",
      "2  1389544            1.415848e+08\n",
      "3  1394472            4.588886e+08\n",
      "4  1395869            2.321004e+08\n",
      "(1281, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = model.best_estimator_.predict(test_b)\n",
    "pred =pd.DataFrame(pred)\n",
    "#best_est = grid.best_estimator_\n",
    "final = pd.concat([test2['key'],pred],axis=1)\n",
    "#final = final.reset_index()\n",
    "final.columns = ['key', 'transaction_real_price']\n",
    "print(final.head())\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"elastic1211_b.csv\", mode='w',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# get the labels\n",
    "y = train.target.values\n",
    "train.drop(['id', 'target'], inplace=True, axis=1)\n",
    "\n",
    "x = train.values\n",
    "\n",
    "# Create training and validation sets\n",
    "x, x_test, y, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create the LightGBM data containers\n",
    "categorical_features = [c for c, col in enumerate(train.columns) if 'cat' in col]\n",
    "train_data = lightgbm.Dataset(x, label=y, categorical_feature=categorical_features)\n",
    "test_data = lightgbm.Dataset(x_test, label=y_test)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "model = lightgbm.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets=test_data,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)\n",
    "\n",
    "# Create a submission\n",
    "submission = pd.read_csv('../input/test.csv')\n",
    "ids = submission['id'].values\n",
    "submission.drop('id', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "x = submission.values\n",
    "y = model.predict(x)\n",
    "\n",
    "output = pd.DataFrame({'id': ids, 'target': y})\n",
    "output.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
